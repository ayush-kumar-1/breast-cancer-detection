{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support\n",
    "\n",
    "import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from load_data import load_train_data, load_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Training Dataset:  11177\n",
      "<ParallelMapDataset element_spec={'image': TensorSpec(shape=(), dtype=tf.string, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None), 'label_normal': TensorSpec(shape=(), dtype=tf.int64, name=None)}>\n",
      "Size of Training Dataset:  11177\n",
      "<ParallelMapDataset element_spec={'image': TensorSpec(shape=(), dtype=tf.string, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None), 'label_normal': TensorSpec(shape=(), dtype=tf.int64, name=None)}>\n",
      "Size of Training Dataset:  11177\n",
      "<ParallelMapDataset element_spec={'image': TensorSpec(shape=(), dtype=tf.string, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None), 'label_normal': TensorSpec(shape=(), dtype=tf.int64, name=None)}>\n",
      "Size of Training Dataset:  11177\n",
      "<ParallelMapDataset element_spec={'image': TensorSpec(shape=(), dtype=tf.string, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None), 'label_normal': TensorSpec(shape=(), dtype=tf.int64, name=None)}>\n",
      "Size of Training Dataset:  11177\n",
      "<ParallelMapDataset element_spec={'image': TensorSpec(shape=(), dtype=tf.string, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None), 'label_normal': TensorSpec(shape=(), dtype=tf.int64, name=None)}>\n"
     ]
    }
   ],
   "source": [
    "train_images, train_labels = load_train_data()\n",
    "test_images, test_labels = load_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55885, 299, 299)\n",
      "(55885,)\n",
      "(15364, 299, 299)\n",
      "(15364,)\n"
     ]
    }
   ],
   "source": [
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)\n",
    "test_images = np.squeeze(test_images, axis=-1)\n",
    "test_labels = (test_labels>0).astype(int)\n",
    "\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "\n",
    "print(test_images.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class NumpyImageDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform or ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.images[idx], self.labels[idx]\n",
    "        image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "train_loader=DataLoader(NumpyImageDataset(train_images, train_labels), \n",
    "    batch_size=32, shuffle=True)\n",
    "    \n",
    "test_loader=DataLoader(NumpyImageDataset(test_images, test_labels),\n",
    "    batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(299*299, 500)\n",
    "        self.fc2 = nn.Linear(500, 100)\n",
    "        self.fc3 = nn.Linear(100, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = nn.functional.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "# Define training function\n",
    "def train(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    y_true = []\n",
    "    y_scores = []\n",
    "    y_pred = []\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        preds = (outputs > 0.5).int()\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        y_true += labels.data.cpu().tolist()\n",
    "        y_scores += outputs.squeeze().data.cpu().tolist()\n",
    "        y_pred += preds.cpu().tolist()\n",
    "\n",
    "        if i+1 % 100 == 0:\n",
    "            auc_roc = roc_auc_score(y_true, y_pred)\n",
    "            precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "            print(f\"Loss: {running_loss/((i+1)*inputs.size(0)):.4f} Acc: {running_corrects.float()/((i+1)*inputs.size(0)):.4f} AUC-ROC: {auc_roc:.4f} Precision: {precision:.4f} Recall: {recall:.4f} F1-score: {f1_score:.4f}\")\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = running_corrects.float() / len(dataloader.dataset)\n",
    "    epoch_auc_roc = roc_auc_score(y_true, y_pred)\n",
    "    epoch_precision, epoch_recall, epoch_f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    print('Train Loss: {:.4f} Acc: {:.4f} AUC-ROC: {:.4f} Precision: {:.4f} Recall: {:.4f} F1-score: {:.4f}'.format(epoch_loss, epoch_acc, epoch_auc_roc, epoch_precision, epoch_recall, epoch_f1_score))\n",
    "    return epoch_loss, epoch_acc, epoch_auc_roc, epoch_precision, epoch_recall, epoch_f1_score\n",
    "\n",
    "# Define evaluation function\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    y_scores = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels.float())\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            preds = (outputs > 0.5).int()\n",
    "            y_scores.append(preds)\n",
    "    \n",
    "    return y_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7317 Acc: 5.0000 AUC-ROC: 0.5000 Precision: 0.1562 Recall: 1.0000 F1-score: 0.2703\n",
      "Loss: 0.4771 Acc: 26.9682 AUC-ROC: 0.5409 Precision: 0.3579 Recall: 0.1111 F1-score: 0.1696\n",
      "Loss: 0.4278 Acc: 26.8944 AUC-ROC: 0.5635 Precision: 0.4784 Recall: 0.1517 F1-score: 0.2304\n",
      "Loss: 0.4050 Acc: 26.9208 AUC-ROC: 0.5700 Precision: 0.5228 Recall: 0.1621 F1-score: 0.2475\n",
      "Train Loss: 0.3996 Acc: 26.8863 AUC-ROC: 0.5740 Precision: 0.5417 Recall: 0.1694 F1-score: 0.2581\n",
      "Loss: 0.3597 Acc: 28.0000 AUC-ROC: 0.5000 Precision: 0.0000 Recall: 0.0000 F1-score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayush/miniconda3/envs/tf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3347 Acc: 27.0186 AUC-ROC: 0.5964 Precision: 0.6493 Recall: 0.2092 F1-score: 0.3164\n",
      "Loss: 0.3331 Acc: 26.8362 AUC-ROC: 0.6012 Precision: 0.6495 Recall: 0.2203 F1-score: 0.3290\n",
      "Loss: 0.3283 Acc: 26.7853 AUC-ROC: 0.6055 Precision: 0.6544 Recall: 0.2292 F1-score: 0.3395\n",
      "Train Loss: 0.3227 Acc: 26.7551 AUC-ROC: 0.6104 Precision: 0.6596 Recall: 0.2393 F1-score: 0.3512\n",
      "Loss: 0.2647 Acc: 29.0000 AUC-ROC: 0.5000 Precision: 0.0000 Recall: 0.0000 F1-score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayush/miniconda3/envs/tf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2908 Acc: 26.6286 AUC-ROC: 0.6313 Precision: 0.6759 Recall: 0.2828 F1-score: 0.3988\n",
      "Loss: 0.2926 Acc: 26.5321 AUC-ROC: 0.6296 Precision: 0.6482 Recall: 0.2823 F1-score: 0.3933\n",
      "Loss: 0.2874 Acc: 26.4981 AUC-ROC: 0.6358 Precision: 0.6552 Recall: 0.2949 F1-score: 0.4067\n",
      "Train Loss: 0.2850 Acc: 26.4885 AUC-ROC: 0.6382 Precision: 0.6626 Recall: 0.2994 F1-score: 0.4124\n",
      "Loss: 0.3317 Acc: 24.1250 AUC-ROC: 0.7500 Precision: 1.0000 Recall: 0.5000 F1-score: 0.6667\n",
      "Loss: 0.2733 Acc: 26.4178 AUC-ROC: 0.6529 Precision: 0.6823 Recall: 0.3287 F1-score: 0.4437\n",
      "Loss: 0.2726 Acc: 26.3384 AUC-ROC: 0.6553 Precision: 0.6834 Recall: 0.3340 F1-score: 0.4487\n",
      "Loss: 0.2736 Acc: 26.3582 AUC-ROC: 0.6512 Precision: 0.6735 Recall: 0.3263 F1-score: 0.4396\n",
      "Train Loss: 0.2725 Acc: 26.3934 AUC-ROC: 0.6508 Precision: 0.6737 Recall: 0.3251 F1-score: 0.4386\n",
      "Loss: 0.2705 Acc: 25.7500 AUC-ROC: 0.6149 Precision: 0.2500 Recall: 0.3333 F1-score: 0.2857\n",
      "Loss: 0.2677 Acc: 26.4824 AUC-ROC: 0.6451 Precision: 0.6496 Recall: 0.3152 F1-score: 0.4245\n",
      "Loss: 0.2713 Acc: 26.3759 AUC-ROC: 0.6460 Precision: 0.6488 Recall: 0.3179 F1-score: 0.4267\n",
      "Loss: 0.2683 Acc: 26.3589 AUC-ROC: 0.6489 Precision: 0.6489 Recall: 0.3242 F1-score: 0.4323\n",
      "Train Loss: 0.2687 Acc: 26.3332 AUC-ROC: 0.6514 Precision: 0.6524 Recall: 0.3291 F1-score: 0.4375\n",
      "Loss: 0.2335 Acc: 27.2500 AUC-ROC: 0.6250 Precision: 1.0000 Recall: 0.2500 F1-score: 0.4000\n",
      "Loss: 0.2562 Acc: 26.4770 AUC-ROC: 0.6651 Precision: 0.6849 Recall: 0.3536 F1-score: 0.4664\n",
      "Loss: 0.2629 Acc: 26.3328 AUC-ROC: 0.6618 Precision: 0.6795 Recall: 0.3481 F1-score: 0.4604\n",
      "Loss: 0.2646 Acc: 26.3510 AUC-ROC: 0.6582 Precision: 0.6762 Recall: 0.3409 F1-score: 0.4533\n",
      "Train Loss: 0.2650 Acc: 26.3249 AUC-ROC: 0.6586 Precision: 0.6718 Recall: 0.3423 F1-score: 0.4535\n",
      "Loss: 0.4293 Acc: 27.3750 AUC-ROC: 0.4655 Precision: 0.0000 Recall: 0.0000 F1-score: 0.0000\n",
      "Loss: 0.2601 Acc: 26.2368 AUC-ROC: 0.6685 Precision: 0.6629 Recall: 0.3646 F1-score: 0.4705\n",
      "Loss: 0.2557 Acc: 26.3417 AUC-ROC: 0.6681 Precision: 0.6674 Recall: 0.3625 F1-score: 0.4698\n",
      "Loss: 0.2590 Acc: 26.2101 AUC-ROC: 0.6704 Precision: 0.6715 Recall: 0.3678 F1-score: 0.4753\n",
      "Train Loss: 0.2595 Acc: 26.2168 AUC-ROC: 0.6701 Precision: 0.6743 Recall: 0.3667 F1-score: 0.4751\n",
      "Loss: 0.1496 Acc: 29.1250 AUC-ROC: 0.7500 Precision: 1.0000 Recall: 0.5000 F1-score: 0.6667\n",
      "Loss: 0.2605 Acc: 26.3956 AUC-ROC: 0.6590 Precision: 0.6705 Recall: 0.3429 F1-score: 0.4537\n",
      "Loss: 0.2629 Acc: 26.3226 AUC-ROC: 0.6595 Precision: 0.6561 Recall: 0.3460 F1-score: 0.4531\n",
      "Loss: 0.2617 Acc: 26.2985 AUC-ROC: 0.6646 Precision: 0.6678 Recall: 0.3556 F1-score: 0.4640\n",
      "Train Loss: 0.2625 Acc: 26.2727 AUC-ROC: 0.6643 Precision: 0.6720 Recall: 0.3546 F1-score: 0.4643\n",
      "Loss: 0.2083 Acc: 24.2500 AUC-ROC: 0.7815 Precision: 0.7500 Recall: 0.6000 F1-score: 0.6667\n",
      "Loss: 0.2591 Acc: 26.2082 AUC-ROC: 0.6778 Precision: 0.6916 Recall: 0.3810 F1-score: 0.4914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1747 [1:49:49<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[84], line 34\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, criterion)\u001b[0m\n\u001b[1;32m     32\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39msqueeze(), labels\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     33\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 34\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     36\u001b[0m preds \u001b[38;5;241m=\u001b[39m (outputs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mint()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    133\u001b[0m         group,\n\u001b[1;32m    134\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    139\u001b[0m         state_steps)\n\u001b[0;32m--> 141\u001b[0m     adam(\n\u001b[1;32m    142\u001b[0m         params_with_grad,\n\u001b[1;32m    143\u001b[0m         grads,\n\u001b[1;32m    144\u001b[0m         exp_avgs,\n\u001b[1;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    147\u001b[0m         state_steps,\n\u001b[1;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    151\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    153\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    158\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/optim/adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 281\u001b[0m func(params,\n\u001b[1;32m    282\u001b[0m      grads,\n\u001b[1;32m    283\u001b[0m      exp_avgs,\n\u001b[1;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    286\u001b[0m      state_steps,\n\u001b[1;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    290\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    292\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/optim/adam.py:344\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    341\u001b[0m     param \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mview_as_real(param)\n\u001b[1;32m    343\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 344\u001b[0m exp_avg\u001b[39m.\u001b[39;49mmul_(beta1)\u001b[39m.\u001b[39;49madd_(grad, alpha\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m beta1)\n\u001b[1;32m    345\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad\u001b[39m.\u001b[39mconj(), value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m capturable \u001b[39mor\u001b[39;00m differentiable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MLP().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "    train(model, train_loader, optimizer, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = evaluate(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6971935007385525, 0.47105788423153694, 0.5622394282310901, None)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = []\n",
    "for tens in y_hat: \n",
    "    y_pred += tens.numpy().flatten().tolist()\n",
    "\n",
    "roc_auc_score(test_labels, y_pred)\n",
    "precision_recall_fscore_support(test_labels, y_pred, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 37 * 37, 256)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7050 Acc: 3.0000 AUC-ROC: 0.5000 Precision: 0.0938 Recall: 1.0000 F1-score: 0.1714\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[100], line 33\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, criterion)\u001b[0m\n\u001b[1;32m     31\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     32\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39msqueeze(), labels\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m---> 33\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     35\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = SimpleCNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "    train(model, train_loader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for tens in y_hat: \n",
    "    y_pred += tens.numpy().flatten().tolist()\n",
    "\n",
    "roc_auc_score(test_labels, y_pred)\n",
    "precision_recall_fscore_support(test_labels, y_pred, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(299*299, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        out = self.linear(x)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6624 Acc: 22.8750 AUC-ROC: 0.4038 Precision: 0.0000 Recall: 0.0000 F1-score: 0.0000\n",
      "Train Loss: 12.9906 Acc: 27.8211 AUC-ROC: 0.4999 Precision: 0.0000 Recall: 0.0000 F1-score: 0.0000\n",
      "Loss: 6.2500 Acc: 30.0000 AUC-ROC: 0.5000 Precision: 0.0000 Recall: 0.0000 F1-score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayush/miniconda3/envs/tf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/ayush/miniconda3/envs/tf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/ayush/miniconda3/envs/tf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 13.0229 Acc: 27.8229 AUC-ROC: 0.5000 Precision: 0.0000 Recall: 0.0000 F1-score: 0.0000\n",
      "Loss: 21.8751 Acc: 25.0000 AUC-ROC: 0.5000 Precision: 0.0000 Recall: 0.0000 F1-score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayush/miniconda3/envs/tf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/ayush/miniconda3/envs/tf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 13.0343 Acc: 27.8225 AUC-ROC: 0.5000 Precision: 0.0000 Recall: 0.0000 F1-score: 0.0000\n",
      "Loss: 15.6250 Acc: 27.0000 AUC-ROC: 0.5000 Precision: 0.0000 Recall: 0.0000 F1-score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayush/miniconda3/envs/tf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/ayush/miniconda3/envs/tf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 13.0400 Acc: 27.8219 AUC-ROC: 0.5000 Precision: 0.0000 Recall: 0.0000 F1-score: 0.0000\n",
      "Loss: 9.3750 Acc: 29.0000 AUC-ROC: 0.5000 Precision: 0.0000 Recall: 0.0000 F1-score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayush/miniconda3/envs/tf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/ayush/miniconda3/envs/tf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 13.0421 Acc: 27.8222 AUC-ROC: 0.5000 Precision: 0.0000 Recall: 0.0000 F1-score: 0.0000\n",
      "Loss: 6.2500 Acc: 30.0000 AUC-ROC: 0.5000 Precision: 0.0000 Recall: 0.0000 F1-score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayush/miniconda3/envs/tf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/ayush/miniconda3/envs/tf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 13.0424 Acc: 27.8222 AUC-ROC: 0.5000 Precision: 0.0000 Recall: 0.0000 F1-score: 0.0000\n",
      "Loss: 6.2500 Acc: 30.0000 AUC-ROC: 0.5000 Precision: 0.0000 Recall: 0.0000 F1-score: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[100], line 31\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, criterion)\u001b[0m\n\u001b[1;32m     29\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     30\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 31\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39msqueeze(), labels\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     33\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[105], line 8\u001b[0m, in \u001b[0;36mLogisticRegression.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m      7\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(out)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = LogisticRegression().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "    train(model, train_loader, optimizer, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63bce780ec3e30c588a92f5285e736f5371ca744aafffd65d76efbb3345ddb49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

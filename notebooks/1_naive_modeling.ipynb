{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "import os \n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(): \n",
    "    training_data = [\"../data/archive/training10_0/training10_0.tfrecords\", \n",
    "        \"../data/archive/training10_1/training10_1.tfrecords\",\n",
    "        \"../data/archive/training10_2/training10_2.tfrecords\",\n",
    "        \"../data/archive/training10_3/training10_3.tfrecords\",\n",
    "        \"../data/archive/training10_4/training10_4.tfrecords\"]\n",
    "\n",
    "    images=[]\n",
    "    labels=[]\n",
    "    feature_dictionary = {\n",
    "        'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'label_normal': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image': tf.io.FixedLenFeature([], tf.string)\n",
    "        }\n",
    "\n",
    "    def _parse_function(example, feature_dictionary=feature_dictionary):\n",
    "        parsed_example = tf.io.parse_example(example, feature_dictionary)\n",
    "        return parsed_example\n",
    "\n",
    "    def read_data(filename):\n",
    "        full_dataset = tf.data.TFRecordDataset(filename,num_parallel_reads=tf.data.experimental.AUTOTUNE)\n",
    "        full_dataset = full_dataset.shuffle(buffer_size=31000)\n",
    "        full_dataset = full_dataset.cache()\n",
    "        print(\"Size of Training Dataset: \", len(list(full_dataset)))\n",
    "        \n",
    "        feature_dictionary = {\n",
    "        'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'label_normal': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image': tf.io.FixedLenFeature([], tf.string)\n",
    "        }   \n",
    "\n",
    "        full_dataset = full_dataset.map(_parse_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "        print(full_dataset)\n",
    "        for image_features in full_dataset:\n",
    "            image = image_features['image'].numpy()\n",
    "            image = tf.io.decode_raw(image_features['image'], tf.uint8)\n",
    "            image = tf.reshape(image, [299, 299])        \n",
    "            image=image.numpy()\n",
    "            #plt.imshow(image)\n",
    "            images.append(image)\n",
    "            labels.append(image_features['label_normal'].numpy())\n",
    "\n",
    "    for file in training_data:\n",
    "        read_data(file)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "def load_test_data():\n",
    "    # Load .npy file\n",
    "    test_data = np.load('../data/archive/test10_data/test10_data.npy')\n",
    "    test_labels = np.load('../data/archive/test10_labels.npy')\n",
    "\n",
    "    cv_data = np.load('../data/archive/cv10_data/cv10_data.npy')\n",
    "    cv_labels = np.load('../data/archive/cv10_labels.npy')\n",
    "\n",
    "    # combine test and cv into single test set\n",
    "    test_data = np.concatenate((test_data, cv_data), axis=0)\n",
    "    test_labels = np.concatenate((test_labels, cv_labels), axis=0)\n",
    "\n",
    "    return test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 14:03:54.617402: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Training Dataset:  11177\n",
      "<ParallelMapDataset element_spec={'image': TensorSpec(shape=(), dtype=tf.string, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None), 'label_normal': TensorSpec(shape=(), dtype=tf.int64, name=None)}>\n",
      "Size of Training Dataset:  11177\n",
      "<ParallelMapDataset element_spec={'image': TensorSpec(shape=(), dtype=tf.string, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None), 'label_normal': TensorSpec(shape=(), dtype=tf.int64, name=None)}>\n",
      "Size of Training Dataset:  11177\n",
      "<ParallelMapDataset element_spec={'image': TensorSpec(shape=(), dtype=tf.string, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None), 'label_normal': TensorSpec(shape=(), dtype=tf.int64, name=None)}>\n",
      "Size of Training Dataset:  11177\n",
      "<ParallelMapDataset element_spec={'image': TensorSpec(shape=(), dtype=tf.string, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None), 'label_normal': TensorSpec(shape=(), dtype=tf.int64, name=None)}>\n",
      "Size of Training Dataset:  11177\n",
      "<ParallelMapDataset element_spec={'image': TensorSpec(shape=(), dtype=tf.string, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None), 'label_normal': TensorSpec(shape=(), dtype=tf.int64, name=None)}>\n"
     ]
    }
   ],
   "source": [
    "train_images, train_labels = load_train_data()\n",
    "test_images, test_labels = load_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55885, 299, 299)\n",
      "(55885,)\n",
      "(15364, 299, 299)\n",
      "(15364,)\n"
     ]
    }
   ],
   "source": [
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)\n",
    "test_images = np.squeeze(test_images, axis=-1)\n",
    "test_labels = (test_labels>0).astype(int)\n",
    "\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "\n",
    "print(test_images.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.backends.mps.is_available())\n",
    "print(torch.backends.mps.is_built())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")\n",
    "\n",
    "class NumpyImageDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform or ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.images[idx], self.labels[idx]\n",
    "        image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "train_loader=DataLoader(NumpyImageDataset(train_images, train_labels), \n",
    "    batch_size=32, shuffle=True)\n",
    "    \n",
    "test_loader=DataLoader(NumpyImageDataset(test_images, test_labels),\n",
    "    batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(299*299, 500)\n",
    "        self.fc2 = nn.Linear(500, 100)\n",
    "        self.fc3 = nn.Linear(100, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = nn.functional.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 37 * 37, 256)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(299*299, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        out = self.linear(x)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function\n",
    "def train(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    y_true = []\n",
    "    y_scores = []\n",
    "    y_pred = []\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(dataloader):\n",
    "        for param in model.parameters():\n",
    "            param.grad = None\n",
    "            \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        preds = (outputs > 0.5).int()\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        y_true += labels.data.cpu().tolist()\n",
    "        y_scores += outputs.squeeze().data.cpu().tolist()\n",
    "        y_pred += preds.cpu().tolist()\n",
    "\n",
    "        if (i % 100 == 0):\n",
    "            auc_roc = roc_auc_score(y_true, y_pred)\n",
    "            precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
    "            print(f\"Loss: {running_loss/((i+1)*inputs.size(0)):.4f} Acc: {running_corrects.float()/((i+1)*inputs.size(0)):.4f} AUC-ROC: {auc_roc:.4f} Precision: {precision:.4f} Recall: {recall:.4f} F1-score: {f1_score:.4f}\")\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = running_corrects.float() / len(dataloader.dataset)\n",
    "    epoch_auc_roc = roc_auc_score(y_true, y_pred)\n",
    "    epoch_precision, epoch_recall, epoch_f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    print('Train Loss: {:.4f} Acc: {:.4f} AUC-ROC: {:.4f} Precision: {:.4f} Recall: {:.4f} F1-score: {:.4f}'.format(epoch_loss, epoch_acc, epoch_auc_roc, epoch_precision, epoch_recall, epoch_f1_score))\n",
    "    return epoch_loss, epoch_acc, epoch_auc_roc, epoch_precision, epoch_recall, epoch_f1_score\n",
    "\n",
    "# Define evaluation function\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    y_scores = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels.float())\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            preds = (outputs > 0.5).int()\n",
    "            y_scores.append(preds)\n",
    "    \n",
    "    return y_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6846 Acc: 27.0000 AUC-ROC: 0.5000 Precision: 0.0000 Recall: 0.0000 F1-score: 0.0000\n",
      "Loss: 0.5690 Acc: 26.5507 AUC-ROC: 0.5209 Precision: 0.2360 Recall: 0.0860 F1-score: 0.1260\n",
      "Loss: 0.4986 Acc: 26.8137 AUC-ROC: 0.5295 Precision: 0.3015 Recall: 0.0919 F1-score: 0.1408\n",
      "Loss: 0.4663 Acc: 26.8721 AUC-ROC: 0.5437 Precision: 0.3844 Recall: 0.1157 F1-score: 0.1779\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, criterion)\u001b[0m\n\u001b[1;32m     18\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 20\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     21\u001b[0m preds \u001b[38;5;241m=\u001b[39m (outputs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mint()\n\u001b[1;32m     22\u001b[0m running_corrects \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(preds \u001b[38;5;241m==\u001b[39m labels\u001b[38;5;241m.\u001b[39mdata)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MLP().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "    train(model, train_loader, optimizer, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6758857285429141\n",
      "(0.7650103519668737, 0.3687624750499002, 0.49764309764309766, None)\n",
      "0.9028898724290549\n"
     ]
    }
   ],
   "source": [
    "y_hat = evaluate(model, test_loader, criterion)\n",
    "y_pred = []\n",
    "for tens in y_hat: \n",
    "    y_pred += tens.numpy().flatten().tolist()\n",
    "\n",
    "print(roc_auc_score(test_labels, y_pred))\n",
    "print(precision_recall_fscore_support(test_labels, y_pred, average='binary'))\n",
    "print(accuracy_score(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7137 Acc: 4.0000 AUC-ROC: 0.5000 Precision: 0.1250 Recall: 1.0000 F1-score: 0.2222\n",
      "Loss: 0.3544 Acc: 27.8144 AUC-ROC: 0.4999 Precision: 0.1212 Recall: 0.0101 F1-score: 0.0186\n",
      "Loss: 0.3263 Acc: 27.7718 AUC-ROC: 0.5113 Precision: 0.3692 Recall: 0.0299 F1-score: 0.0554\n",
      "Loss: 0.3212 Acc: 27.3698 AUC-ROC: 0.5445 Precision: 0.5319 Recall: 0.1020 F1-score: 0.1712\n",
      "Loss: 0.3196 Acc: 27.0567 AUC-ROC: 0.5655 Precision: 0.5781 Recall: 0.1473 F1-score: 0.2347\n",
      "Loss: 0.3170 Acc: 26.9508 AUC-ROC: 0.5761 Precision: 0.5812 Recall: 0.1706 F1-score: 0.2638\n",
      "Loss: 0.3142 Acc: 26.8443 AUC-ROC: 0.5868 Precision: 0.6032 Recall: 0.1929 F1-score: 0.2923\n",
      "Loss: 0.3108 Acc: 26.7824 AUC-ROC: 0.5937 Precision: 0.6036 Recall: 0.2081 F1-score: 0.3095\n",
      "Loss: 0.3061 Acc: 26.7888 AUC-ROC: 0.5986 Precision: 0.6121 Recall: 0.2178 F1-score: 0.3213\n",
      "Loss: 0.3039 Acc: 26.7807 AUC-ROC: 0.6003 Precision: 0.6187 Recall: 0.2209 F1-score: 0.3255\n",
      "Loss: 0.3016 Acc: 26.7432 AUC-ROC: 0.6061 Precision: 0.6266 Recall: 0.2329 F1-score: 0.3396\n",
      "Loss: 0.3007 Acc: 26.6848 AUC-ROC: 0.6096 Precision: 0.6271 Recall: 0.2407 F1-score: 0.3478\n",
      "Loss: 0.2967 Acc: 26.6594 AUC-ROC: 0.6140 Precision: 0.6342 Recall: 0.2496 F1-score: 0.3582\n",
      "Loss: 0.2947 Acc: 26.6114 AUC-ROC: 0.6179 Precision: 0.6366 Recall: 0.2579 F1-score: 0.3671\n",
      "Loss: 0.2917 Acc: 26.5728 AUC-ROC: 0.6231 Precision: 0.6439 Recall: 0.2687 F1-score: 0.3791\n",
      "Loss: 0.2910 Acc: 26.5374 AUC-ROC: 0.6255 Precision: 0.6466 Recall: 0.2736 F1-score: 0.3845\n",
      "Loss: 0.2889 Acc: 26.5313 AUC-ROC: 0.6280 Precision: 0.6479 Recall: 0.2789 F1-score: 0.3900\n",
      "Loss: 0.2872 Acc: 26.5209 AUC-ROC: 0.6308 Precision: 0.6522 Recall: 0.2845 F1-score: 0.3961\n",
      "Train Loss: 0.2855 Acc: 26.5165 AUC-ROC: 0.6330 Precision: 0.6547 Recall: 0.2888 F1-score: 0.4008\n",
      "Loss: 0.1943 Acc: 27.3750 AUC-ROC: 0.6494 Precision: 0.5000 Recall: 0.3333 F1-score: 0.4000\n",
      "Loss: 0.2437 Acc: 26.2240 AUC-ROC: 0.6936 Precision: 0.7414 Recall: 0.4086 F1-score: 0.5268\n",
      "Loss: 0.2535 Acc: 26.4683 AUC-ROC: 0.6704 Precision: 0.7233 Recall: 0.3612 F1-score: 0.4818\n",
      "Loss: 0.2532 Acc: 26.5141 AUC-ROC: 0.6628 Precision: 0.7065 Recall: 0.3466 F1-score: 0.4651\n",
      "Loss: 0.2516 Acc: 26.3867 AUC-ROC: 0.6731 Precision: 0.7037 Recall: 0.3692 F1-score: 0.4843\n",
      "Loss: 0.2529 Acc: 26.3312 AUC-ROC: 0.6735 Precision: 0.6991 Recall: 0.3708 F1-score: 0.4846\n",
      "Loss: 0.2530 Acc: 26.3435 AUC-ROC: 0.6727 Precision: 0.7032 Recall: 0.3686 F1-score: 0.4836\n",
      "Loss: 0.2542 Acc: 26.3103 AUC-ROC: 0.6728 Precision: 0.6969 Recall: 0.3695 F1-score: 0.4829\n",
      "Loss: 0.2546 Acc: 26.3068 AUC-ROC: 0.6731 Precision: 0.7022 Recall: 0.3697 F1-score: 0.4844\n",
      "Loss: 0.2539 Acc: 26.3233 AUC-ROC: 0.6720 Precision: 0.7054 Recall: 0.3670 F1-score: 0.4828\n",
      "Loss: 0.2529 Acc: 26.3551 AUC-ROC: 0.6718 Precision: 0.7093 Recall: 0.3660 F1-score: 0.4828\n",
      "Loss: 0.2520 Acc: 26.3728 AUC-ROC: 0.6721 Precision: 0.7104 Recall: 0.3664 F1-score: 0.4834\n",
      "Loss: 0.2524 Acc: 26.3364 AUC-ROC: 0.6737 Precision: 0.7136 Recall: 0.3696 F1-score: 0.4869\n",
      "Loss: 0.2528 Acc: 26.3057 AUC-ROC: 0.6748 Precision: 0.7150 Recall: 0.3720 F1-score: 0.4894\n",
      "Loss: 0.2524 Acc: 26.2992 AUC-ROC: 0.6744 Precision: 0.7151 Recall: 0.3710 F1-score: 0.4885\n",
      "Loss: 0.2514 Acc: 26.2928 AUC-ROC: 0.6754 Precision: 0.7169 Recall: 0.3731 F1-score: 0.4907\n",
      "Loss: 0.2511 Acc: 26.3070 AUC-ROC: 0.6757 Precision: 0.7191 Recall: 0.3734 F1-score: 0.4915\n",
      "Loss: 0.2502 Acc: 26.3039 AUC-ROC: 0.6769 Precision: 0.7212 Recall: 0.3756 F1-score: 0.4939\n",
      "Train Loss: 0.2500 Acc: 26.3154 AUC-ROC: 0.6761 Precision: 0.7215 Recall: 0.3739 F1-score: 0.4925\n",
      "Loss: 0.2740 Acc: 26.5000 AUC-ROC: 0.6071 Precision: 0.5000 Recall: 0.2500 F1-score: 0.3333\n",
      "Loss: 0.2367 Acc: 26.2073 AUC-ROC: 0.7066 Precision: 0.7811 Recall: 0.4313 F1-score: 0.5557\n",
      "Loss: 0.2337 Acc: 26.2183 AUC-ROC: 0.7038 Precision: 0.7634 Recall: 0.4272 F1-score: 0.5478\n",
      "Loss: 0.2406 Acc: 26.0401 AUC-ROC: 0.7047 Precision: 0.7544 Recall: 0.4310 F1-score: 0.5486\n",
      "Loss: 0.2431 Acc: 26.1093 AUC-ROC: 0.6995 Precision: 0.7516 Recall: 0.4202 F1-score: 0.5391\n",
      "Loss: 0.2444 Acc: 26.2282 AUC-ROC: 0.6924 Precision: 0.7432 Recall: 0.4057 F1-score: 0.5249\n",
      "Loss: 0.2432 Acc: 26.1938 AUC-ROC: 0.6975 Precision: 0.7480 Recall: 0.4160 F1-score: 0.5347\n",
      "Loss: 0.2427 Acc: 26.1912 AUC-ROC: 0.6992 Precision: 0.7529 Recall: 0.4191 F1-score: 0.5385\n",
      "Loss: 0.2417 Acc: 26.2046 AUC-ROC: 0.6993 Precision: 0.7561 Recall: 0.4190 F1-score: 0.5392\n",
      "Loss: 0.2402 Acc: 26.2060 AUC-ROC: 0.7010 Precision: 0.7598 Recall: 0.4220 F1-score: 0.5426\n",
      "Loss: 0.2398 Acc: 26.2636 AUC-ROC: 0.6986 Precision: 0.7591 Recall: 0.4168 F1-score: 0.5382\n",
      "Loss: 0.2403 Acc: 26.2177 AUC-ROC: 0.7005 Precision: 0.7604 Recall: 0.4208 F1-score: 0.5418\n",
      "Loss: 0.2396 Acc: 26.1753 AUC-ROC: 0.7016 Precision: 0.7596 Recall: 0.4233 F1-score: 0.5436\n",
      "Loss: 0.2399 Acc: 26.1481 AUC-ROC: 0.7029 Precision: 0.7613 Recall: 0.4260 F1-score: 0.5463\n",
      "Loss: 0.2396 Acc: 26.1448 AUC-ROC: 0.7024 Precision: 0.7602 Recall: 0.4251 F1-score: 0.5453\n",
      "Loss: 0.2388 Acc: 26.1751 AUC-ROC: 0.7020 Precision: 0.7599 Recall: 0.4242 F1-score: 0.5445\n",
      "Loss: 0.2391 Acc: 26.1986 AUC-ROC: 0.6996 Precision: 0.7586 Recall: 0.4193 F1-score: 0.5401\n",
      "Loss: 0.2389 Acc: 26.2259 AUC-ROC: 0.6985 Precision: 0.7613 Recall: 0.4167 F1-score: 0.5386\n",
      "Train Loss: 0.2394 Acc: 26.2213 AUC-ROC: 0.6983 Precision: 0.7597 Recall: 0.4164 F1-score: 0.5379\n",
      "Loss: 0.2516 Acc: 25.0000 AUC-ROC: 0.7143 Precision: 0.5000 Recall: 0.5000 F1-score: 0.5000\n",
      "Loss: 0.2455 Acc: 26.4220 AUC-ROC: 0.6802 Precision: 0.7659 Recall: 0.3774 F1-score: 0.5056\n",
      "Loss: 0.2398 Acc: 26.5090 AUC-ROC: 0.6844 Precision: 0.7715 Recall: 0.3853 F1-score: 0.5139\n",
      "Loss: 0.2394 Acc: 26.5571 AUC-ROC: 0.6813 Precision: 0.7603 Recall: 0.3799 F1-score: 0.5066\n",
      "Loss: 0.2358 Acc: 26.5617 AUC-ROC: 0.6858 Precision: 0.7497 Recall: 0.3899 F1-score: 0.5130\n",
      "Loss: 0.2383 Acc: 26.4111 AUC-ROC: 0.6886 Precision: 0.7500 Recall: 0.3964 F1-score: 0.5186\n",
      "Loss: 0.2382 Acc: 26.4059 AUC-ROC: 0.6892 Precision: 0.7560 Recall: 0.3971 F1-score: 0.5207\n",
      "Loss: 0.2394 Acc: 26.4248 AUC-ROC: 0.6876 Precision: 0.7612 Recall: 0.3932 F1-score: 0.5186\n",
      "Loss: 0.2379 Acc: 26.3734 AUC-ROC: 0.6918 Precision: 0.7621 Recall: 0.4019 F1-score: 0.5263\n",
      "Loss: 0.2361 Acc: 26.3520 AUC-ROC: 0.6936 Precision: 0.7620 Recall: 0.4059 F1-score: 0.5297\n",
      "Loss: 0.2366 Acc: 26.3044 AUC-ROC: 0.6950 Precision: 0.7572 Recall: 0.4095 F1-score: 0.5315\n",
      "Loss: 0.2360 Acc: 26.2622 AUC-ROC: 0.6979 Precision: 0.7569 Recall: 0.4156 F1-score: 0.5366\n",
      "Loss: 0.2354 Acc: 26.2785 AUC-ROC: 0.6979 Precision: 0.7584 Recall: 0.4155 F1-score: 0.5368\n",
      "Loss: 0.2347 Acc: 26.2944 AUC-ROC: 0.6978 Precision: 0.7573 Recall: 0.4152 F1-score: 0.5363\n",
      "Loss: 0.2347 Acc: 26.2729 AUC-ROC: 0.6980 Precision: 0.7573 Recall: 0.4157 F1-score: 0.5368\n",
      "Loss: 0.2347 Acc: 26.2905 AUC-ROC: 0.6969 Precision: 0.7581 Recall: 0.4134 F1-score: 0.5350\n",
      "Loss: 0.2344 Acc: 26.2606 AUC-ROC: 0.6989 Precision: 0.7611 Recall: 0.4173 F1-score: 0.5390\n",
      "Loss: 0.2340 Acc: 26.2189 AUC-ROC: 0.7014 Precision: 0.7635 Recall: 0.4223 F1-score: 0.5438\n",
      "Train Loss: 0.2346 Acc: 26.2157 AUC-ROC: 0.7003 Precision: 0.7644 Recall: 0.4201 F1-score: 0.5422\n",
      "Loss: 0.2625 Acc: 25.6250 AUC-ROC: 0.7000 Precision: 1.0000 Recall: 0.4000 F1-score: 0.5714\n",
      "Loss: 0.2214 Acc: 26.2073 AUC-ROC: 0.7307 Precision: 0.7756 Recall: 0.4817 F1-score: 0.5943\n",
      "Loss: 0.2215 Acc: 26.3703 AUC-ROC: 0.7251 Precision: 0.7874 Recall: 0.4681 F1-score: 0.5871\n",
      "Loss: 0.2269 Acc: 26.3106 AUC-ROC: 0.7137 Precision: 0.7749 Recall: 0.4463 F1-score: 0.5664\n",
      "Loss: 0.2254 Acc: 26.3217 AUC-ROC: 0.7129 Precision: 0.7742 Recall: 0.4444 F1-score: 0.5647\n",
      "Loss: 0.2289 Acc: 26.2308 AUC-ROC: 0.7125 Precision: 0.7767 Recall: 0.4437 F1-score: 0.5648\n",
      "Loss: 0.2285 Acc: 26.2318 AUC-ROC: 0.7142 Precision: 0.7762 Recall: 0.4473 F1-score: 0.5675\n",
      "Loss: 0.2275 Acc: 26.2227 AUC-ROC: 0.7140 Precision: 0.7790 Recall: 0.4467 F1-score: 0.5678\n",
      "Loss: 0.2271 Acc: 26.2045 AUC-ROC: 0.7124 Precision: 0.7749 Recall: 0.4438 F1-score: 0.5644\n",
      "Loss: 0.2275 Acc: 26.1458 AUC-ROC: 0.7150 Precision: 0.7786 Recall: 0.4492 F1-score: 0.5697\n",
      "Loss: 0.2291 Acc: 26.1449 AUC-ROC: 0.7133 Precision: 0.7743 Recall: 0.4460 F1-score: 0.5660\n",
      "Loss: 0.2299 Acc: 26.1366 AUC-ROC: 0.7114 Precision: 0.7711 Recall: 0.4425 F1-score: 0.5623\n",
      "Loss: 0.2282 Acc: 26.1987 AUC-ROC: 0.7100 Precision: 0.7731 Recall: 0.4393 F1-score: 0.5602\n",
      "Loss: 0.2289 Acc: 26.2103 AUC-ROC: 0.7091 Precision: 0.7734 Recall: 0.4373 F1-score: 0.5587\n",
      "Loss: 0.2299 Acc: 26.2027 AUC-ROC: 0.7080 Precision: 0.7703 Recall: 0.4354 F1-score: 0.5564\n",
      "Loss: 0.2306 Acc: 26.1970 AUC-ROC: 0.7075 Precision: 0.7713 Recall: 0.4343 F1-score: 0.5557\n",
      "Loss: 0.2304 Acc: 26.2007 AUC-ROC: 0.7085 Precision: 0.7737 Recall: 0.4361 F1-score: 0.5578\n",
      "Loss: 0.2305 Acc: 26.1911 AUC-ROC: 0.7091 Precision: 0.7756 Recall: 0.4371 F1-score: 0.5591\n",
      "Train Loss: 0.2304 Acc: 26.1697 AUC-ROC: 0.7100 Precision: 0.7756 Recall: 0.4392 F1-score: 0.5608\n"
     ]
    }
   ],
   "source": [
    "model = SimpleCNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "for epoch in range(5):\n",
    "    train(model, train_loader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.1608 Acc: 23.5625 AUC-ROC: 0.8815 Precision: 0.8000 Recall: 0.8000 F1-score: 0.8000\n",
      "Loss: 0.1781 Acc: 25.6572 AUC-ROC: 0.7691 Precision: 0.7888 Recall: 0.5610 F1-score: 0.6557\n",
      "Loss: 0.1797 Acc: 25.7484 AUC-ROC: 0.7670 Precision: 0.8003 Recall: 0.5550 F1-score: 0.6555\n",
      "Loss: 0.1822 Acc: 25.6773 AUC-ROC: 0.7664 Precision: 0.7875 Recall: 0.5557 F1-score: 0.6516\n",
      "Loss: 0.1840 Acc: 25.5525 AUC-ROC: 0.7712 Precision: 0.7817 Recall: 0.5669 F1-score: 0.6572\n",
      "Loss: 0.1821 Acc: 25.5947 AUC-ROC: 0.7720 Precision: 0.7823 Recall: 0.5683 F1-score: 0.6583\n",
      "Loss: 0.1811 Acc: 25.6585 AUC-ROC: 0.7721 Precision: 0.7889 Recall: 0.5672 F1-score: 0.6599\n",
      "Loss: 0.1811 Acc: 25.6864 AUC-ROC: 0.7731 Precision: 0.7939 Recall: 0.5685 F1-score: 0.6626\n",
      "Loss: 0.1820 Acc: 25.6616 AUC-ROC: 0.7736 Precision: 0.7945 Recall: 0.5697 F1-score: 0.6636\n",
      "Loss: 0.1803 Acc: 25.6887 AUC-ROC: 0.7746 Precision: 0.7932 Recall: 0.5718 F1-score: 0.6645\n",
      "Loss: 0.1796 Acc: 25.7241 AUC-ROC: 0.7740 Precision: 0.7937 Recall: 0.5703 F1-score: 0.6637\n",
      "Loss: 0.1800 Acc: 25.7144 AUC-ROC: 0.7746 Precision: 0.7950 Recall: 0.5713 F1-score: 0.6648\n",
      "Loss: 0.1810 Acc: 25.7014 AUC-ROC: 0.7755 Precision: 0.7973 Recall: 0.5729 F1-score: 0.6667\n",
      "Loss: 0.1806 Acc: 25.7012 AUC-ROC: 0.7761 Precision: 0.7965 Recall: 0.5744 F1-score: 0.6674\n",
      "Loss: 0.1815 Acc: 25.6813 AUC-ROC: 0.7762 Precision: 0.7961 Recall: 0.5747 F1-score: 0.6675\n",
      "Loss: 0.1817 Acc: 25.7040 AUC-ROC: 0.7764 Precision: 0.7977 Recall: 0.5748 F1-score: 0.6681\n",
      "Loss: 0.1810 Acc: 25.7120 AUC-ROC: 0.7768 Precision: 0.7975 Recall: 0.5755 F1-score: 0.6686\n",
      "Loss: 0.1805 Acc: 25.7339 AUC-ROC: 0.7762 Precision: 0.7983 Recall: 0.5742 F1-score: 0.6680\n",
      "Train Loss: 0.1802 Acc: 25.7276 AUC-ROC: 0.7765 Precision: 0.7982 Recall: 0.5748 F1-score: 0.6684\n",
      "Loss: 0.0915 Acc: 22.8750 AUC-ROC: 0.9815 Precision: 0.8333 Recall: 1.0000 F1-score: 0.9091\n",
      "Loss: 0.1578 Acc: 25.4839 AUC-ROC: 0.8161 Precision: 0.8070 Recall: 0.6556 F1-score: 0.7235\n",
      "Loss: 0.1620 Acc: 25.6984 AUC-ROC: 0.8042 Precision: 0.8072 Recall: 0.6304 F1-score: 0.7079\n",
      "Loss: 0.1657 Acc: 25.7319 AUC-ROC: 0.7974 Precision: 0.8105 Recall: 0.6159 F1-score: 0.7000\n",
      "Loss: 0.1677 Acc: 25.6122 AUC-ROC: 0.7948 Precision: 0.8038 Recall: 0.6121 F1-score: 0.6949\n",
      "Loss: 0.1688 Acc: 25.5631 AUC-ROC: 0.7948 Precision: 0.7975 Recall: 0.6130 F1-score: 0.6932\n",
      "Loss: 0.1698 Acc: 25.5921 AUC-ROC: 0.7922 Precision: 0.7975 Recall: 0.6076 F1-score: 0.6897\n",
      "Loss: 0.1715 Acc: 25.6445 AUC-ROC: 0.7893 Precision: 0.7970 Recall: 0.6014 F1-score: 0.6855\n",
      "Loss: 0.1718 Acc: 25.6645 AUC-ROC: 0.7873 Precision: 0.7980 Recall: 0.5972 F1-score: 0.6831\n",
      "Loss: 0.1708 Acc: 25.6605 AUC-ROC: 0.7883 Precision: 0.8017 Recall: 0.5988 F1-score: 0.6856\n",
      "Loss: 0.1723 Acc: 25.6359 AUC-ROC: 0.7871 Precision: 0.8004 Recall: 0.5966 F1-score: 0.6836\n",
      "Loss: 0.1723 Acc: 25.6649 AUC-ROC: 0.7857 Precision: 0.7993 Recall: 0.5938 F1-score: 0.6814\n",
      "Loss: 0.1727 Acc: 25.6667 AUC-ROC: 0.7869 Precision: 0.8004 Recall: 0.5961 F1-score: 0.6833\n",
      "Loss: 0.1724 Acc: 25.6619 AUC-ROC: 0.7878 Precision: 0.8019 Recall: 0.5977 F1-score: 0.6849\n",
      "Loss: 0.1747 Acc: 25.6500 AUC-ROC: 0.7871 Precision: 0.8034 Recall: 0.5962 F1-score: 0.6845\n",
      "Loss: 0.1751 Acc: 25.6659 AUC-ROC: 0.7858 Precision: 0.8033 Recall: 0.5935 F1-score: 0.6827\n",
      "Loss: 0.1762 Acc: 25.6681 AUC-ROC: 0.7848 Precision: 0.8035 Recall: 0.5913 F1-score: 0.6813\n",
      "Loss: 0.1754 Acc: 25.6621 AUC-ROC: 0.7855 Precision: 0.8037 Recall: 0.5928 F1-score: 0.6823\n",
      "Train Loss: 0.1754 Acc: 25.6848 AUC-ROC: 0.7843 Precision: 0.8041 Recall: 0.5902 F1-score: 0.6808\n",
      "Loss: 0.1292 Acc: 25.7500 AUC-ROC: 0.8750 Precision: 1.0000 Recall: 0.7500 F1-score: 0.8571\n",
      "Loss: 0.1756 Acc: 25.6238 AUC-ROC: 0.7885 Precision: 0.8247 Recall: 0.5962 F1-score: 0.6921\n",
      "Loss: 0.1763 Acc: 25.5541 AUC-ROC: 0.7921 Precision: 0.8069 Recall: 0.6061 F1-score: 0.6923\n",
      "Loss: 0.1686 Acc: 25.6144 AUC-ROC: 0.7940 Precision: 0.8271 Recall: 0.6073 F1-score: 0.7003\n",
      "Loss: 0.1694 Acc: 25.5968 AUC-ROC: 0.7949 Precision: 0.8160 Recall: 0.6107 F1-score: 0.6986\n",
      "Loss: 0.1699 Acc: 25.6679 AUC-ROC: 0.7878 Precision: 0.8138 Recall: 0.5961 F1-score: 0.6881\n",
      "Loss: 0.1689 Acc: 25.6313 AUC-ROC: 0.7902 Precision: 0.8142 Recall: 0.6011 F1-score: 0.6916\n",
      "Loss: 0.1670 Acc: 25.6470 AUC-ROC: 0.7922 Precision: 0.8104 Recall: 0.6056 F1-score: 0.6932\n",
      "Loss: 0.1691 Acc: 25.6081 AUC-ROC: 0.7927 Precision: 0.8105 Recall: 0.6068 F1-score: 0.6940\n",
      "Loss: 0.1691 Acc: 25.6340 AUC-ROC: 0.7925 Precision: 0.8110 Recall: 0.6062 F1-score: 0.6938\n",
      "Loss: 0.1693 Acc: 25.5986 AUC-ROC: 0.7947 Precision: 0.8120 Recall: 0.6107 F1-score: 0.6971\n",
      "Loss: 0.1692 Acc: 25.6231 AUC-ROC: 0.7943 Precision: 0.8105 Recall: 0.6099 F1-score: 0.6960\n",
      "Loss: 0.1695 Acc: 25.5925 AUC-ROC: 0.7956 Precision: 0.8104 Recall: 0.6128 F1-score: 0.6979\n",
      "Loss: 0.1680 Acc: 25.6027 AUC-ROC: 0.7966 Precision: 0.8113 Recall: 0.6148 F1-score: 0.6995\n",
      "Loss: 0.1678 Acc: 25.6140 AUC-ROC: 0.7965 Precision: 0.8082 Recall: 0.6148 F1-score: 0.6983\n",
      "Loss: 0.1676 Acc: 25.6330 AUC-ROC: 0.7960 Precision: 0.8094 Recall: 0.6135 F1-score: 0.6980\n",
      "Loss: 0.1679 Acc: 25.6353 AUC-ROC: 0.7958 Precision: 0.8078 Recall: 0.6134 F1-score: 0.6973\n",
      "Loss: 0.1683 Acc: 25.6212 AUC-ROC: 0.7961 Precision: 0.8081 Recall: 0.6140 F1-score: 0.6978\n",
      "Train Loss: 0.1692 Acc: 25.6127 AUC-ROC: 0.7953 Precision: 0.8074 Recall: 0.6126 F1-score: 0.6966\n",
      "Loss: 0.1986 Acc: 22.1875 AUC-ROC: 0.7657 Precision: 0.8000 Recall: 0.5714 F1-score: 0.6667\n",
      "Loss: 0.1619 Acc: 25.8700 AUC-ROC: 0.7960 Precision: 0.8164 Recall: 0.6118 F1-score: 0.6994\n",
      "Loss: 0.1585 Acc: 25.7994 AUC-ROC: 0.7980 Precision: 0.8230 Recall: 0.6152 F1-score: 0.7041\n",
      "Loss: 0.1605 Acc: 25.6591 AUC-ROC: 0.8026 Precision: 0.8162 Recall: 0.6261 F1-score: 0.7086\n",
      "Loss: 0.1579 Acc: 25.7344 AUC-ROC: 0.8035 Precision: 0.8196 Recall: 0.6270 F1-score: 0.7105\n",
      "Loss: 0.1580 Acc: 25.6719 AUC-ROC: 0.8044 Precision: 0.8162 Recall: 0.6296 F1-score: 0.7109\n",
      "Loss: 0.1646 Acc: 25.6549 AUC-ROC: 0.8034 Precision: 0.8149 Recall: 0.6279 F1-score: 0.7093\n",
      "Loss: 0.1661 Acc: 25.6736 AUC-ROC: 0.8002 Precision: 0.8112 Recall: 0.6218 F1-score: 0.7040\n",
      "Loss: 0.1661 Acc: 25.6749 AUC-ROC: 0.8009 Precision: 0.8124 Recall: 0.6230 F1-score: 0.7052\n",
      "Loss: 0.1660 Acc: 25.6349 AUC-ROC: 0.8015 Precision: 0.8123 Recall: 0.6245 F1-score: 0.7061\n",
      "Loss: 0.1658 Acc: 25.6358 AUC-ROC: 0.8020 Precision: 0.8124 Recall: 0.6255 F1-score: 0.7068\n",
      "Loss: 0.1661 Acc: 25.6378 AUC-ROC: 0.8028 Precision: 0.8128 Recall: 0.6270 F1-score: 0.7079\n",
      "Loss: 0.1657 Acc: 25.6166 AUC-ROC: 0.8025 Precision: 0.8158 Recall: 0.6262 F1-score: 0.7085\n",
      "Loss: 0.1655 Acc: 25.6165 AUC-ROC: 0.8037 Precision: 0.8151 Recall: 0.6287 F1-score: 0.7099\n",
      "Loss: 0.1651 Acc: 25.6203 AUC-ROC: 0.8032 Precision: 0.8136 Recall: 0.6277 F1-score: 0.7087\n",
      "Loss: 0.1657 Acc: 25.6188 AUC-ROC: 0.8021 Precision: 0.8141 Recall: 0.6255 F1-score: 0.7074\n",
      "Loss: 0.1653 Acc: 25.5771 AUC-ROC: 0.8037 Precision: 0.8148 Recall: 0.6288 F1-score: 0.7098\n",
      "Loss: 0.1650 Acc: 25.5878 AUC-ROC: 0.8030 Precision: 0.8150 Recall: 0.6274 F1-score: 0.7090\n",
      "Train Loss: 0.1646 Acc: 25.5830 AUC-ROC: 0.8040 Precision: 0.8165 Recall: 0.6293 F1-score: 0.7108\n",
      "Loss: 0.1290 Acc: 27.3750 AUC-ROC: 0.8333 Precision: 1.0000 Recall: 0.6667 F1-score: 0.8000\n",
      "Loss: 0.1424 Acc: 25.4783 AUC-ROC: 0.8328 Precision: 0.8559 Recall: 0.6831 F1-score: 0.7598\n",
      "Loss: 0.1594 Acc: 25.5780 AUC-ROC: 0.8107 Precision: 0.8310 Recall: 0.6410 F1-score: 0.7237\n",
      "Loss: 0.1587 Acc: 25.5322 AUC-ROC: 0.8100 Precision: 0.8254 Recall: 0.6407 F1-score: 0.7214\n",
      "Loss: 0.1581 Acc: 25.5678 AUC-ROC: 0.8074 Precision: 0.8302 Recall: 0.6345 F1-score: 0.7193\n",
      "Loss: 0.1558 Acc: 25.5836 AUC-ROC: 0.8104 Precision: 0.8277 Recall: 0.6408 F1-score: 0.7224\n",
      "Loss: 0.1540 Acc: 25.5858 AUC-ROC: 0.8125 Precision: 0.8286 Recall: 0.6450 F1-score: 0.7254\n",
      "Loss: 0.1542 Acc: 25.5780 AUC-ROC: 0.8118 Precision: 0.8242 Recall: 0.6441 F1-score: 0.7231\n",
      "Loss: 0.1552 Acc: 25.5996 AUC-ROC: 0.8099 Precision: 0.8251 Recall: 0.6402 F1-score: 0.7210\n",
      "Loss: 0.1550 Acc: 25.5864 AUC-ROC: 0.8117 Precision: 0.8282 Recall: 0.6433 F1-score: 0.7241\n",
      "Loss: 0.1549 Acc: 25.5581 AUC-ROC: 0.8137 Precision: 0.8305 Recall: 0.6472 F1-score: 0.7275\n",
      "Loss: 0.1534 Acc: 25.5963 AUC-ROC: 0.8143 Precision: 0.8285 Recall: 0.6486 F1-score: 0.7276\n",
      "Loss: 0.1540 Acc: 25.6039 AUC-ROC: 0.8132 Precision: 0.8276 Recall: 0.6465 F1-score: 0.7259\n",
      "Loss: 0.1556 Acc: 25.5689 AUC-ROC: 0.8128 Precision: 0.8253 Recall: 0.6460 F1-score: 0.7247\n",
      "Loss: 0.1559 Acc: 25.5604 AUC-ROC: 0.8116 Precision: 0.8238 Recall: 0.6439 F1-score: 0.7228\n",
      "Loss: 0.1547 Acc: 25.5814 AUC-ROC: 0.8115 Precision: 0.8249 Recall: 0.6433 F1-score: 0.7229\n",
      "Loss: 0.1554 Acc: 25.5584 AUC-ROC: 0.8111 Precision: 0.8232 Recall: 0.6429 F1-score: 0.7220\n",
      "Loss: 0.1553 Acc: 25.5572 AUC-ROC: 0.8124 Precision: 0.8246 Recall: 0.6454 F1-score: 0.7241\n",
      "Train Loss: 0.1556 Acc: 25.5479 AUC-ROC: 0.8123 Precision: 0.8237 Recall: 0.6454 F1-score: 0.7237\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    train(model, train_loader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7493138722554891\n",
      "(0.90625, 0.5064870259481038, 0.6498079385403329, None)\n",
      "0.9287945847435564\n"
     ]
    }
   ],
   "source": [
    "y_hat = evaluate(model, test_loader, criterion)\n",
    "y_pred = []\n",
    "for tens in y_hat: \n",
    "    tens = tens.to('cpu')\n",
    "    y_pred += tens.numpy().flatten().tolist()\n",
    "\n",
    "print(roc_auc_score(test_labels, y_pred))\n",
    "print(precision_recall_fscore_support(test_labels, y_pred, average='binary'))\n",
    "print(accuracy_score(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7355 Acc: 7.2500 AUC-ROC: 0.6000 Precision: 0.0769 Recall: 1.0000 F1-score: 0.1429\n",
      "Loss: 13.1086 Acc: 27.5173 AUC-ROC: 0.4980 Precision: 0.0769 Recall: 0.0047 F1-score: 0.0088\n",
      "Loss: 12.7673 Acc: 27.7674 AUC-ROC: 0.4991 Precision: 0.0769 Recall: 0.0024 F1-score: 0.0047\n",
      "Loss: 12.7527 Acc: 27.8181 AUC-ROC: 0.4994 Precision: 0.0769 Recall: 0.0016 F1-score: 0.0032\n",
      "Loss: 12.7119 Acc: 27.8560 AUC-ROC: 0.4995 Precision: 0.0769 Recall: 0.0012 F1-score: 0.0024\n",
      "Loss: 12.8468 Acc: 27.8268 AUC-ROC: 0.4996 Precision: 0.0769 Recall: 0.0010 F1-score: 0.0019\n",
      "Loss: 12.9041 Acc: 27.8157 AUC-ROC: 0.4997 Precision: 0.0769 Recall: 0.0008 F1-score: 0.0016\n",
      "Loss: 12.8597 Acc: 27.8363 AUC-ROC: 0.4997 Precision: 0.0769 Recall: 0.0007 F1-score: 0.0014\n",
      "Loss: 12.8963 Acc: 27.8293 AUC-ROC: 0.4998 Precision: 0.0769 Recall: 0.0006 F1-score: 0.0012\n",
      "Loss: 12.9882 Acc: 27.8038 AUC-ROC: 0.4998 Precision: 0.0769 Recall: 0.0005 F1-score: 0.0011\n",
      "Loss: 13.0089 Acc: 27.7994 AUC-ROC: 0.4998 Precision: 0.0769 Recall: 0.0005 F1-score: 0.0010\n",
      "Loss: 13.0049 Acc: 27.8031 AUC-ROC: 0.4998 Precision: 0.0769 Recall: 0.0004 F1-score: 0.0009\n",
      "Loss: 13.0322 Acc: 27.7962 AUC-ROC: 0.4998 Precision: 0.0769 Recall: 0.0004 F1-score: 0.0008\n",
      "Loss: 12.9158 Acc: 27.8357 AUC-ROC: 0.4999 Precision: 0.0769 Recall: 0.0004 F1-score: 0.0007\n",
      "Loss: 12.9198 Acc: 27.8353 AUC-ROC: 0.4999 Precision: 0.0769 Recall: 0.0003 F1-score: 0.0007\n",
      "Loss: 12.9863 Acc: 27.8150 AUC-ROC: 0.4999 Precision: 0.0769 Recall: 0.0003 F1-score: 0.0006\n",
      "Loss: 13.0074 Acc: 27.8084 AUC-ROC: 0.4999 Precision: 0.0769 Recall: 0.0003 F1-score: 0.0006\n",
      "Loss: 12.9987 Acc: 27.8126 AUC-ROC: 0.4999 Precision: 0.0769 Recall: 0.0003 F1-score: 0.0006\n",
      "Train Loss: 12.9979 Acc: 27.8095 AUC-ROC: 0.4999 Precision: 0.0769 Recall: 0.0003 F1-score: 0.0005\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "for epoch in range(1):\n",
    "    train(model, train_loader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "(0.0, 0.0, 0.0, None)\n",
      "0.8695652173913043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayush/miniconda3/envs/tf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_hat = evaluate(model, test_loader, criterion)\n",
    "y_pred = []\n",
    "for tens in y_hat: \n",
    "    y_pred += tens.numpy().flatten().tolist()\n",
    "\n",
    "print(roc_auc_score(test_labels, y_pred))\n",
    "print(precision_recall_fscore_support(test_labels, y_pred, average='binary'))\n",
    "print(accuracy_score(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting einops\n",
      "  Using cached einops-0.6.1-py3-none-any.whl (42 kB)\n",
      "Installing collected packages: einops\n",
      "Successfully installed einops-0.6.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "%pip install einops\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, image_size=299, patch_size=16, num_classes=2, dim=768, depth=12, heads=12, mlp_dim=3072):\n",
    "        super().__init__()\n",
    "        assert image_size % patch_size == 0, \"image size must be divisible by patch size\"\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        patch_dim = 3 * patch_size ** 2  # assuming 3-channel RGB images\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        \n",
    "        # input projection\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(1, dim, kernel_size=patch_size, stride=patch_size),\n",
    "            Rearrange('b c h w -> b (h w) c')\n",
    "        )\n",
    "        \n",
    "        # transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "        \n",
    "        # output projection\n",
    "        self.fc = nn.Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # pad the input image to make it evenly divisible by patch_size\n",
    "        _, _, h, w = x.size()\n",
    "        ph = (self.patch_size - h % self.patch_size) % self.patch_size\n",
    "        pw = (self.patch_size - w % self.patch_size) % self.patch_size\n",
    "        padding = nn.ZeroPad2d((0, pw, 0, ph))\n",
    "        x = padding(x)\n",
    "        \n",
    "        x = self.projection(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1/1], Loss: 0.6198\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 15.21 GB, other allocations: 2.92 GB, max allowed: 18.13 GB). Tried to allocate 33.84 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[1;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[26], line 38\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m x \u001b[38;5;241m=\u001b[39m padding(x)\n\u001b[1;32m     37\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojection(x)\n\u001b[0;32m---> 38\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     40\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    303\u001b[0m is_causal \u001b[39m=\u001b[39m make_causal\n\u001b[1;32m    305\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m--> 306\u001b[0m     output \u001b[39m=\u001b[39m mod(output, src_mask\u001b[39m=\u001b[39;49mmask, is_causal\u001b[39m=\u001b[39;49mis_causal, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask_for_layers)\n\u001b[1;32m    308\u001b[0m \u001b[39mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    309\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mto_padded_tensor(\u001b[39m0.\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/transformer.py:573\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    571\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x))\n\u001b[1;32m    572\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 573\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sa_block(x, src_mask, src_key_padding_mask))\n\u001b[1;32m    574\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(x))\n\u001b[1;32m    576\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/transformer.py:585\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sa_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor,\n\u001b[1;32m    580\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    581\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn(x, x, x,\n\u001b[1;32m    582\u001b[0m                        attn_mask\u001b[39m=\u001b[39mattn_mask,\n\u001b[1;32m    583\u001b[0m                        key_padding_mask\u001b[39m=\u001b[39mkey_padding_mask,\n\u001b[1;32m    584\u001b[0m                        need_weights\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m--> 585\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout1(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/torch/nn/functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[1;32m   1251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(p))\n\u001b[0;32m-> 1252\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, p, training)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 15.21 GB, other allocations: 2.92 GB, max allowed: 18.13 GB). Tried to allocate 33.84 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "model = VisionTransformer(image_size=320, patch_size=16).to(device)\n",
    "\n",
    "# Step 3: Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Step 4: Write a training loop\n",
    "num_epochs = 10\n",
    "total_steps = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        total_steps += 1\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and update the parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                    .format(epoch+1, 10, i+1, total_steps, loss.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63bce780ec3e30c588a92f5285e736f5371ca744aafffd65d76efbb3345ddb49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
